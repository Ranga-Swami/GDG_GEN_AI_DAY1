{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcd//yhelRRpHWn9DWkc7G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec - Easily Explained\n",
        "\n",
        "=> The main drawback of BOW(Bag of Words) and TF-IDF is that the semantic information(ie meaning of the word is not considered) and the order in which the words occur in a sentence is not preserved.\n",
        "\n",
        "=> This might lead to overfitting.\n",
        "\n",
        "Thus, Word2Vec is used to convert the words to 32 or more dimensional vectors by considering the semantic as well as the order in which the words appear\n",
        "\n",
        "For example, In a sentence that has the words \"man\" and \"woman\"\n",
        "Representing in a 2D space, the word \"man\" can have a vector - (8, 6)\n",
        "Representing in a 2D space, the word \"woman\" can have a vector - (8.2, 6.4)\n",
        "since the word \"woman\" has the sub-word \"man\", the two vectors can be interrelated and the difference is minimal. For the word \"hello\", it might be (10, 14)\n",
        "\n",
        "STEPS TO CREATE WORD2VEC\n",
        "\n",
        "* Tokenize the sentences\n",
        "* Create histograms\n",
        "* Identify the most frequent words\n",
        "* Build a matrix with all unique words"
      ],
      "metadata": {
        "id": "MQKkOiX5s-uG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peRQEC9oqzMH",
        "outputId": "b135df2e-265a-4587-e3fb-34b5837451d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#implementing word2vec vusing gensim module\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkTgk96btPqn",
        "outputId": "aabafcfd-ce19-4880-e943-b5e81651ba66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWbHeB6ntPvl",
        "outputId": "4d964d4d-fbf1-495b-9883-ba1fd1b0aef3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "                Ferrari endured a disastrous end to qualifying when Sainz hit the wall at the final corner as he prepared to\n",
        "                start a lap, putting him 10th, with Charles Leclerc only ninth thanks to his time being deleted for exceeding\n",
        "                track limits.\n",
        "                Nico Hulkenberg was a fine sixth for Haas in another trademark qualifying performance from the German,\n",
        "                while Aston Martin’s Fernando Alonso and RB’s Yuki Tsunoda also capitalised on the Scuderia’s troubles\n",
        "                to finish P7 and P8.\n",
        "                \"\"\""
      ],
      "metadata": {
        "id": "7hmThZkutbZ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# text = re.sub(r'[0-9]*', ' ', paragraph)  #remove digit\n",
        "text = re.sub(r'\\s+', ' ', paragraph) #remove special characters\n",
        "text = text.lower()    #convert text to lower case\n",
        "text = re.sub(r'\\d', ' ', text)\n",
        "text = re.sub(r'\\s+', ' ', text)"
      ],
      "metadata": {
        "id": "3mc9957-tbhc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "f_81uqUGtrYK",
        "outputId": "cf1ba736-8f82-43e0-ee4e-6177cf29bbe3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ferrari endured a disastrous end to qualifying when sainz hit the wall at the final corner as he prepared to start a lap, putting him th, with charles leclerc only ninth thanks to his time being deleted for exceeding track limits. nico hulkenberg was a fine sixth for haas in another trademark qualifying performance from the german, while aston martin’s fernando alonso and rb’s yuki tsunoda also capitalised on the scuderia’s troubles to finish p and p . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "Ewz6Oy_Ztrns"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TblDmzsbtyQI",
        "outputId": "8cf5c48b-f003-4e6b-ca67-de9dd27a01e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ferrari',\n",
              "  'endured',\n",
              "  'disastrous',\n",
              "  'end',\n",
              "  'qualifying',\n",
              "  'sainz',\n",
              "  'hit',\n",
              "  'wall',\n",
              "  'final',\n",
              "  'corner',\n",
              "  'prepared',\n",
              "  'start',\n",
              "  'lap',\n",
              "  ',',\n",
              "  'putting',\n",
              "  'th',\n",
              "  ',',\n",
              "  'charles',\n",
              "  'leclerc',\n",
              "  'ninth',\n",
              "  'thanks',\n",
              "  'time',\n",
              "  'deleted',\n",
              "  'exceeding',\n",
              "  'track',\n",
              "  'limits',\n",
              "  '.'],\n",
              " ['nico',\n",
              "  'hulkenberg',\n",
              "  'fine',\n",
              "  'sixth',\n",
              "  'haas',\n",
              "  'another',\n",
              "  'trademark',\n",
              "  'qualifying',\n",
              "  'performance',\n",
              "  'german',\n",
              "  ',',\n",
              "  'aston',\n",
              "  'martin',\n",
              "  '’',\n",
              "  'fernando',\n",
              "  'alonso',\n",
              "  'rb',\n",
              "  '’',\n",
              "  'yuki',\n",
              "  'tsunoda',\n",
              "  'also',\n",
              "  'capitalised',\n",
              "  'scuderia',\n",
              "  '’',\n",
              "  'troubles',\n",
              "  'finish',\n",
              "  'p',\n",
              "  'p',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}